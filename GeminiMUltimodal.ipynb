{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxSf5V4vSvTVDKEF/ixxBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araujobma/LangchainGeminiMultimodalMongoDB/blob/main/GeminiMUltimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Multimodal RAG Applications with LangChain\n",
        "## This notebook is a replication of the notebook from this video https://www.youtube.com/watch?v=vxF8-ay9Bzk (Google Cloud Events) with some adaptations.\n",
        "Use at your own risk ðŸ˜†. You won't need to pay for any service\n",
        "\n",
        "The changes are:\n",
        "  - The vectorstore is changed(adapted) to Atlas mongoDB\n",
        "  - Instead of VertexAI, it is used langchain-google-genai to access models(llm chat and embedding)\n",
        "  - the code is not completely the same. It has some minors changes like variable names etc."
      ],
      "metadata": {
        "id": "cYmKDSF_oWWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing the installing step:"
      ],
      "metadata": {
        "id": "1umkVH5RrNHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain-google-genai langchain-mongodb pymongo[srv] pypdf"
      ],
      "metadata": {
        "id": "UH_Q3uLlrSuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First you need to set the anvironment variable GOOGLE_API_KEY; This key you can generate at https://aistudio.google.com for free. This is necessary to use Gemini"
      ],
      "metadata": {
        "id": "e28f9KlqsHvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\") #this is defined in colab Secrets"
      ],
      "metadata": {
        "id": "aKiHwfkrrYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First part of video: Langchain basics"
      ],
      "metadata": {
        "id": "pyJvvAk2tHsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.2)"
      ],
      "metadata": {
        "id": "320reg35r9mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"What is BMW revenue for 2022 break it up per line of business and share the 2024 business prespective?\"\n",
        "for chunk in chat_model.stream([question1]):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "PNURFfxatYUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"What is {company} revenue for {year} break it up per line of business and share the 2024 business perspective\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | chat_model #model__or__(prompt)\n",
        "chain"
      ],
      "metadata": {
        "id": "4BbV2VRFtdoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chain.stream({\"company\": \"BMW\", \"year\": 2022}):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Zi-i5vBotkZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"company\": \"BMW\", \"year\": 2022})\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "qm6P7dhKtnkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm_model.invoke(question1, stream=True, stop=[\"growth\"]):\n",
        "    print(chunk[1], end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "bvCzdCf3trOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat_model.invoke([HumanMessage(content=question1)])\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "SmosXyQKtt-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chat_model.stream([HumanMessage(content=question1)]):\n",
        "    print(chunk.content)"
      ],
      "metadata": {
        "id": "-quokQLXuKPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat_model.invoke([HumanMessage(content=\"Hello. Who are you?\")])"
      ],
      "metadata": {
        "id": "QsPBm47AuNXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer.content)"
      ],
      "metadata": {
        "id": "EWvTGFTLuRHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat_model.invoke([HumanMessage(content=\"Are you multimodal? Or should I use another model\")])\n",
        "#When I run this cell the llm answered that is not a Multimodal. But in fact the model gemini-1.5-flash is multimodal\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "J0nOpHH7uTSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "import base64\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"\"getting base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "  \"\"\"Display base64 encoded string as image\"\"\"\n",
        "  #Create an HTML img tag with the base64 string as the source\n",
        "  image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
        "  #Display the image by rendering the HTML\n",
        "  display(HTML(image_html))"
      ],
      "metadata": {
        "id": "zZntBNWrurgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcRKJRziB51kfnSanSzZLUIAPOUP4vhYSbs2MN8-CEGMzjHXGfOLpD-J7ORjvVdGskFXRYmc3l15\" #ai studio example you can use another\n",
        "\n",
        "import requests\n",
        "\n",
        "with open('image.jpeg', 'wb') as f:\n",
        "    f.write(requests.get(img_url).content)\n",
        "\n"
      ],
      "metadata": {
        "id": "rzohQ5NVxDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(encode_image('image.jpeg'))"
      ],
      "metadata": {
        "id": "ob8Ptop_z1GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"What's in this image?\",\n",
        "        },  # You can optionally provide text parts\n",
        "        {\"type\": \"image_url\", \"image_url\": './image.jpeg'},\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer = chat_model.invoke([message])\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "JpxzySeI0EA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second part. The RAG part\n",
        "you need this file: storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/cj.zip"
      ],
      "metadata": {
        "id": "Wdh0Utb01ABi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"./cj.pdf\")\n",
        "docs = loader.load()\n",
        "tables =[]\n",
        "texts = [d.page_content for d in docs]"
      ],
      "metadata": {
        "id": "8ELwXqma0i4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "id": "Ug7QtLrA1Hui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "izh36KgR2xOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries of text elements\n",
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "    \"\"\"\n",
        "    texts: List of str\n",
        "    tables: List of str\n",
        "    summarize_texts: Bool\n",
        "    \"\"\"\n",
        "\n",
        "    #Prompt\n",
        "    prompt_text = \"\"\"\n",
        "    You are an assistant tasked with summarizing tables and text for retrieval.\n",
        "    These summaries will be embedded  and used  to retrieve the raw text or table elements.\n",
        "    Give a concise  summary of the table or text that is well  optimized  for retrieval.\n",
        "    Table or text: {element}\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(prompt_text)\n",
        "    empty_response = RunnableLambda(\n",
        "        lambda x: AIMessage(content=\"Error processing document\")\n",
        "    )\n",
        "    #Text summary chain\n",
        "    model = ChatGoogleGenerativeAI(\n",
        "        temperature=0,\n",
        "        model='gemini-1.5-flash-latest',\n",
        "        max_output_tokens=1024\n",
        "    ).with_fallbacks([empty_response])\n",
        "    #summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "    summarize_chain =  prompt | model | StrOutputParser()\n",
        "\n",
        "    #Initialize empty summaries\n",
        "    text_summaries = []\n",
        "    table_summaries = []\n",
        "\n",
        "    #Apply to text  if text are provided and summarization is requested\n",
        "    if texts and summarize_texts:\n",
        "        #text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "        text_summaries = [summarize_chain.invoke(text) for text in texts]\n",
        "    elif texts:\n",
        "        text_summaries = texts\n",
        "\n",
        "    # Apply to tables if tables are provided\n",
        "    if tables:\n",
        "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "    return text_summaries, table_summaries\n"
      ],
      "metadata": {
        "id": "_Ylyk3Lw20ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This will raise some excessive calls to API but will finish\n",
        "text_summaries, table_summaries = generate_text_summaries(texts, tables, summarize_texts=True)"
      ],
      "metadata": {
        "id": "GZry1FOW26_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_summaries))\n",
        "text_summaries[0:2]"
      ],
      "metadata": {
        "id": "RtVg7hwf3H_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_summirize(img_base64, prompt):\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model='gemini-1.5-flash-latest', max_output_tokens=1024)\n",
        "    msg = model(\n",
        "        [\n",
        "            HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "                {\n",
        "                  \"type\": \"image_url\",\n",
        "                  \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                },\n",
        "            ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        ""
      ],
      "metadata": {
        "id": "fwpAltwE3UQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_img_summaries(path):\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    #Store base64 encoded images\n",
        "    img_base64_list = []\n",
        "    #Store image summaries\n",
        "    image_summaries = []\n",
        "\n",
        "    #Prompt\n",
        "    prompt = \"\"\"\n",
        "    You are an assistant tasked with summarizing images for retrieval.\n",
        "    These summaries  will be embedded and used to retrieve the raw image.\n",
        "    Give concise summary of the image that is well optimized for retrieval.\n",
        "    \"\"\"\n",
        "\n",
        "    #Apply to images\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith('.jpg'):\n",
        "            img_path =  os.path.join(path, img_file)\n",
        "            base64_img = encode_image(img_path)\n",
        "            img_base64_list.append(base64_img)\n",
        "            image_summaries.append(image_summirize(base64_img, prompt))\n",
        "    return img_base64_list, image_summaries\n",
        "\n",
        "#Image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(\"./\")"
      ],
      "metadata": {
        "id": "8nf08JyS3ubL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(img_base64_list))\n",
        "image_summaries\n"
      ],
      "metadata": {
        "id": "0xwOF2zY32Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "#from langchain_community.vectorstores import Chroma\n",
        "from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "L_T6mjek3_UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you need to set this env variable\n",
        "os.environ[\"MONGODB_CONNECTION_STRING\"] = userdata.get(\"MONGODB_CONNECTION_STRING\")\n",
        "#You can create a free cluster in atlas mongodb and you nedd to allow access\n",
        "#from anywhere to acces the cluster from colab\n",
        "\n",
        "from pymongo import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import os\n",
        "mongo_client = MongoClient(os.environ[\"MONGODB_CONNECTION_STRING\"], server_api=ServerApi('1'))\n",
        "try:\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "7WLg2uNq4C9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multi_vector_retriever(\n",
        "    vectorstore, store, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Create retriever that indexes summaries, but retruns raw image or texts\n",
        "    \"\"\"\n",
        "\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    #Create the multivector retriever\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        docstore=store,\n",
        "        id_key=id_key\n",
        "    )\n",
        "\n",
        "    #Helper function to add documents to the vectorstore and docstore\n",
        "    def add_documents(retriever, doc_summaries, doc_contents):\n",
        "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "        summary_docs = [\n",
        "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "            for i, s in enumerate(doc_summaries)\n",
        "        ]\n",
        "        retriever.vectorstore.add_documents(summary_docs)\n",
        "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "    ##Add texts, tables and images\n",
        "    #Check that text_summaries is not empty before adding\n",
        "    if text_summaries:\n",
        "        add_documents(retriever, text_summaries, texts)\n",
        "    #Check that table_summaries is not empty before adding\n",
        "    if table_summaries:\n",
        "        add_documents(retriever, table_summaries, tables)\n",
        "    #Check that image_summares is not empty before adding\n",
        "    if image_summaries:\n",
        "        add_documents(retriever, image_summaries, images)\n",
        "\n",
        "    return retriever\n",
        ""
      ],
      "metadata": {
        "id": "KWeDnGRe463Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to create the database and collection both with same name \"mm_rag_cj_blog\"(thats what is used below)\n",
        "https://gist.github.com/araujobma/d495097fe3d19fd9e5735b25b5903ff0\n"
      ],
      "metadata": {
        "id": "6ZCq7VKt7VQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the information below to create a vector search index in Atlas MongoDB(where you can create a free cluster - local mongodb community doesn't have search index)\n",
        "# {\n",
        "#         \"fields\": [\n",
        "#             {\n",
        "#               \"type\": \"vector\",\n",
        "#               \"path\": \"embedding\",\n",
        "#               \"numDimensions\": 768,\n",
        "#               \"similarity\": \"dotProduct\"\n",
        "#             },\n",
        "\n",
        "\n",
        "#        ]\n",
        "#    }\n",
        "\n",
        "#    name=\"mm_rag_cj_blog_vector_index\""
      ],
      "metadata": {
        "id": "-rkT2A-l9Mcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The vectorstore to use to index the summaries\n",
        "db_collection= mongo_client[\"mm_rag_cj_blog\"][\"mm_rag_cj_blog\"]\n",
        "vectostore = MongoDBAtlasVectorSearch(\n",
        "    db_collection,\n",
        "    GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
        "    index_name='mm_rag_cj_blog_vector_index',\n",
        "    relevance_score_fn='dotProduct'\n",
        ")\n",
        "\n",
        "#Initialize the storage layer\n",
        "inmemory_store = InMemoryStore()"
      ],
      "metadata": {
        "id": "5xqtiz8S7Gdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector_image = create_multi_vector_retriever(\n",
        "    vectostore,\n",
        "    inmemory_store,\n",
        "    text_summaries,\n",
        "    texts,\n",
        "    table_summaries,\n",
        "    tables,\n",
        "    image_summaries,\n",
        "    img_base64_list\n",
        ")"
      ],
      "metadata": {
        "id": "Mo9JEKv0_IWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string  looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "def is_image_data(b64data):\n",
        "    \"\"\"\n",
        "    Check if the base64 data is an image by looking at the start of the data\n",
        "    \"\"\"\n",
        "\n",
        "    image_signatures = {\n",
        "            b\"\\xFF\\xD8\\xFF\\xE0\\x00\\x10\\x4A\\x46\\x49\\x46\": \"JPEG\",\n",
        "            b\"\\x89PNG\\r\\n\\x1a\\n\": \"PNG\",\n",
        "            b\"\\x47\\x49\\x46\\x38\\x37\\x61\": \"GIF\",\n",
        "            b\"\\x42\\x4D\": \"BMP\",\n",
        "            b\"\\x49\\x49\\x2A\\x00\": \"TIFF\",\n",
        "            b\"\\x52\\x49\\x46\\x46\": \"WebP\",\n",
        "            b\"\\x00\\x00\\x01\\x00\": \"ICO\",\n",
        "            b\"\\x49\\x43\\x43\\x50\": \"ICNS\",\n",
        "        }\n",
        "    try:\n",
        "        header = base64.b64encode(b64data)[:8]\n",
        "        for sig, format in image_signatures.items():\n",
        "            if header.startswith(sig):\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def resize_base64_image(base64_string, size=(128,128)):\n",
        "  \"\"\"\n",
        "  Resize an image encoded as  Base64 string\n",
        "  \"\"\"\n",
        "  #Decode the Base64 string\n",
        "  img_data = base64.b64decode(base64_string)\n",
        "  img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "  #Resize the image\n",
        "  resized_image = img.resize(size, Image.LANCZOS)\n",
        "  #Save the resized image to bytes buffer\n",
        "  buffered =  io.BytesIO()\n",
        "  resized_image.save(buffered, format=img.format)\n",
        "\n",
        "  #Encode the resized image to Base64\n",
        "  return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "    \"\"\"\n",
        "    Split base64-encoded images and texts\n",
        "    \"\"\"\n",
        "    b64_images = []\n",
        "    texts = []\n",
        "    for doc in docs:\n",
        "        if isinstance(doc, Document):\n",
        "            doc = doc.page_content\n",
        "        if looks_like_base64(doc) and is_image_data(doc):\n",
        "            doc = resize_base64_image(doc, size=(1300,600))\n",
        "            b64_images.append(doc)\n",
        "        else:\n",
        "            texts.append(doc)\n",
        "    if len(b64_images) > 0:\n",
        "        return {\"images\": b64_images[:1], \"texts\": []}\n",
        "    return {\"images\": b64_images, \"texts\": texts}\n",
        "\n",
        "\n",
        "def img_prompt_func(data_dict):\n",
        "  \"\"\"\n",
        "  Join the context into a single string\n",
        "  \"\"\"\n",
        "  formatted_texts = '\\n'.join(data_dict[\"context\"][\"texts\"])\n",
        "  messages = []\n",
        "\n",
        "  #Adding the text for analysis\n",
        "  text_message = {\n",
        "      \"type\": \"text\",\n",
        "      \"text\": f\"\"\"\n",
        "      You are financial analyst tasking with providing investiment advice.\\n\n",
        "      You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\n",
        "      Use this information to provide investiment advice related to user question. \\n\n",
        "      User-provided question: {data_dict['question']}\\n\\n\n",
        "      {formatted_texts}\"\"\"\n",
        "\n",
        "\n",
        "  }\n",
        "\n",
        "  messages.append(text_message)\n",
        "  # Adding image(s) to the messages if present\n",
        "  if data_dict[\"context\"][\"images\"]:\n",
        "    for image in data_dict[\"context\"][\"images\"]:\n",
        "      image_message = {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "      }\n",
        "      messages.append(image_message)\n",
        "  return [HumanMessage(content=messages)]\n",
        "\n",
        "def multi_modal_rag_chain(retriever):\n",
        "  #Multimodal LLM\n",
        "  model = ChatGoogleGenerativeAI(\n",
        "      temperature=0, model='gemini-1.5-flash-latest',\n",
        "      max_output_tokens=1024\n",
        "  )\n",
        "\n",
        "  #RAG pipeline\n",
        "  chain = (\n",
        "      {\n",
        "          'context': retriever | RunnableLambda(split_image_text_types),\n",
        "          'question': RunnablePassthrough(),\n",
        "      }\n",
        "      | RunnableLambda(img_prompt_func)\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  return chain\n",
        "\n",
        "#Create RAG chain\n",
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_image)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9tYsV2rv_LIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the EV/NTM and NTM rev growth for MongoDB, Cloudflare, and DataDog?\"\n",
        "docs = retriever_multi_vector_image.get_relevant_documents(query, limit=1)\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "id": "s1_amUKcIxuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(docs[0])"
      ],
      "metadata": {
        "id": "xTPJHdSxJsvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain_multimodal_rag.invoke(query)"
      ],
      "metadata": {
        "id": "SJr-bRTnKDlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown as md\n",
        "md(result)"
      ],
      "metadata": {
        "id": "SyeENzpqKp4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain_multimodal_rag.invoke(\"What are the EV / NTM rev growth for adobe\")\n",
        "md(result)"
      ],
      "metadata": {
        "id": "XI6VI_HvLjDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3g9HGluL8V9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}